# CNN_and_basics_of_regularization

## Part 1: Optimizers

Previously, we have covered the Stochastic Gradient Descent (SGD) optimization method. In this task, more complex methods will be covered, and you will implement them in numpy: SGD with Momentum, RMSprop and Adam.

## Part 2: Regularization

To efficienlty train a good neural network, using a suitable optimizer is important but not enough. Other important considerations include how to select good initialization values for ANN parameters, and how to avoid overfitting. This task addresses some of the regularization methods, such as **Dropout** and **Batch Normalization**.

## Part 3: Convolutional Neural Network (CNN)

In contrast to the Multi-Layer Perceptron (ML) or fully-connected (FC) nets, Convolutional Neural Netwworks are a more powerful tool often used in the field of computer vision. In this task, you are going to create a CNN.

## Task 4: Data Augmentation

Data augmentation is very useful for CNNs. It can enrich the collected datasets by enlarging them using the operations on the original data samples. It can promote the performance of networks.

## Task 5: Bottle Kaggle Competition

This requires to join an in-class Kaggle competition and compete with your classmates on the "bottle classification problem". You need to keep updating your CNN in iterations to get the best possible result. Kaggle will enable you to observe the progress made by students in the class. You can use any of the deep learning modelling techiques (such as regularization and optimization). You have to use TensorFlow to build your model.
